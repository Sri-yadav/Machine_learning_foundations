{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac462478",
   "metadata": {},
   "source": [
    "\n",
    "## What is a Neural Network ? \n",
    "\n",
    "- Just like regression and SVM, it is also a **function**:\n",
    "  $$\n",
    "  y = f_{NN}(x)\n",
    "  $$\n",
    "  (**a nested function**)\n",
    "\n",
    "- Eg. 3-layer NN:\n",
    "  $$\n",
    "  y = f_3(f_2(f_1(x)))\n",
    "  $$\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ce7a65",
   "metadata": {},
   "source": [
    "## What is a layer?\n",
    "\n",
    "- Each layer is also a function. It is made up of many units (also called neurons)\n",
    "  $$\n",
    "  f_l(z) = g_l(W_l z + b_l)\n",
    "  $$\n",
    "- Where:\n",
    "  - $z$: input vector (that each neuron recieves)\n",
    "  - $W_l$: weight matrix (whose each row $w_{l,u}$ in $W_l$ is a weight vector for one unit)\n",
    "  - $b_l$: bias vector\n",
    "  - $g_l$: activation function (non-linear)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02ae06f",
   "metadata": {},
   "source": [
    "\n",
    "- Each unit (or neuron) computes:\n",
    "  $$\n",
    "  a_{l,u} = w_{l,u}z + b_{l,u}\n",
    "  $$\n",
    "  $$\n",
    "  \\text{Output: } g_l(a_{l,u})\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3dc01a",
   "metadata": {},
   "source": [
    "\n",
    "- **Layer Output Vector**\n",
    "\n",
    "  - For all units in a layer :\n",
    "    $$\n",
    "    [g_l(a_{l,1}), g_l(a_{l,2}), ..., g_l(a_{l,\\text{size}_l})]\n",
    "    $$\n",
    "  - This becomes the input to the next layer.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b8b092",
   "metadata": {},
   "source": [
    "# Neural Network Architecture\n",
    "Neural network architectures are the structural designs that define how the layers in a neural network are arrange, how data flows through them, types of layers (like fully connected, convolutional, recurrent), their order, connections, and activation functions.  \n",
    "\n",
    "For diffferent task different architectures are used. We'll look at some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125384f5",
   "metadata": {},
   "source": [
    "\n",
    "## Multilayer Perceptron (MLP)\n",
    "\n",
    "- A type of Feed-Forward Neural Network (FFNN)\n",
    "- **Fully-connected**\n",
    "- with one or more hidden layers, and an output layer and no special (eg. convolutional layer) layers is called : **Vanilla Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e044e8",
   "metadata": {},
   "source": [
    "\n",
    "## How Units Work\n",
    "\n",
    "- Input → Vector\n",
    "- Apply:\n",
    "  1. **Linear transformation**: $w \\cdot x + b$\n",
    "  2. **Activation**: $g(w \\cdot x + b)$\n",
    "- Output sent to next layer’s all units (fully connected)\n",
    "\n",
    "<img src=\"Images/MLP_6.png\" width= 500 >\n",
    "\n",
    "- Each unit = circle or rectangle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199502fe",
   "metadata": {},
   "source": [
    "\n",
    "- **FFNN Output Layer**\n",
    "\n",
    "  - Last layer decides task:\n",
    "    - **Regression** → Linear activation\n",
    "    - **Binary Classification** → Logistic activation\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8985cffe",
   "metadata": {},
   "source": [
    "\n",
    "## Why Activation Functions Matter\n",
    "\n",
    "- Without them → entire NN becomes **linear**\n",
    "- Linear function of a linear function = linear\n",
    "- Nonlinear activations let NNs learn complex patterns\n",
    "\n",
    "---\n",
    "\n",
    "## Common Activation Functions\n",
    "\n",
    "### Logistic (Sigmoid)\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "### TanH\n",
    "$$\n",
    "\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
    "$$\n",
    "\n",
    "### ReLU\n",
    "$$\n",
    "\\text{ReLU}(z) =\n",
    "\\begin{cases}\n",
    "0 & \\text{if } z < 0 \\\\\n",
    "z & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61efa6d7",
   "metadata": {},
   "source": [
    "\n",
    "## Layer Computation Recap\n",
    "\n",
    "- Compute:\n",
    "  $$\n",
    "  a_l = W_l z\n",
    "  $$\n",
    "- Add bias:\n",
    "  $$\n",
    "  c_l = a_l + b_l\n",
    "  $$\n",
    "- Apply activation:\n",
    "  $$\n",
    "  y_l = g_l(c_l)\n",
    "  $$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5d2dbe",
   "metadata": {},
   "source": [
    "## What is Deep Learning?\n",
    "\n",
    "- Training neural networks with **more than two non-output layers**.\n",
    "- Earlier, training deep networks was difficult due to:\n",
    "  - **Exploding gradient problem**\n",
    "  - **Vanishing gradient problem**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e8b16c",
   "metadata": {},
   "source": [
    "\n",
    "## Exploding vs Vanishing Gradient\n",
    "\n",
    "- **Exploding Gradient:**\n",
    "  - During backpropagation, gradients grow uncontrollably large which causes unstable updates that may lead to weight overflow\n",
    "  - Easier to handle (compared to vanishing)\n",
    "  - Techniques: Gradient clipping, L1/L2 regularization\n",
    "\n",
    "- **Vanishing Gradient:**\n",
    "  - More challenging\n",
    "  - Causes very small gradients → parameters stop updating → training halts\n",
    "\n",
    "### Why Vanishing Gradient Happens\n",
    "\n",
    "- Neural networks are trained using **backpropagation**:\n",
    "  - Uses the **chain rule** to compute gradients layer by layer.\n",
    "- Traditional activations (e.g., hyperbolic tangent) have gradients in (0,1).\n",
    "- Multiplying many small gradients across layers → gradient shrinks **exponentially** with depth.\n",
    "- Result: Early layers train very slowly or not at all.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44652c3",
   "metadata": {},
   "source": [
    "\n",
    "## Modern Solutions to Vanishing Gradient\n",
    "\n",
    "- Use of **ReLU** activation, which reduces vanishing gradient effect.\n",
    "- Architectures like **LSTM** and **Residual Networks** (with skip connections).\n",
    "- These allow training of very deep networks (hundreds or thousands of layers).\n",
    "\n",
    "- \"Deep learning\" now means training neural networks using modern techniques regardless of depth.\n",
    "\n",
    "- **Hidden layers** = layers that are neither input nor output.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d7a159",
   "metadata": {},
   "source": [
    "\n",
    "# Convolutional Neural Network (CNN)\n",
    "(another neural network architecture used specially for images)\n",
    "\n",
    "### Why CNNs?\n",
    "\n",
    "- MLPs grow **very fast** in parameters with more layers.\n",
    "- Adding a 1000-unit layer adds **over 1 million parameters**.\n",
    "- Image inputs are **high-dimensional**, making MLPs **hard to optimize**.\n",
    "- CNNs reduce parameters drastically **without losing much accuracy**.\n",
    "- Especially useful in **image and text processing**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f62f273",
   "metadata": {},
   "source": [
    "\n",
    "## Intuition Behind CNNs\n",
    "\n",
    "- Nearby pixels in images often represent the **same type of info** (eg, sky, water, etc).\n",
    "- Exceptions are edges where different objects meet.\n",
    "- CNNs learn to detect **regions and edges** to recognize objects.\n",
    "- Example: detecting skin regions + edges (blue colour) → likely a face on sky background.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e23f96",
   "metadata": {},
   "source": [
    "\n",
    "## How CNNs Work: Moving Window Approach\n",
    "\n",
    "- Split image into **small square patches**.\n",
    "- Train multiple small regression models on patches.\n",
    "- Each model detects a **specific pattern** (sky, grass, edges).\n",
    "- Each model learns parameters of a **filter matrix F** (e.g., 3×3).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7741a409",
   "metadata": {},
   "source": [
    "\n",
    "## CNN Layer Structure\n",
    "\n",
    "- One CNN layer = multiple filters + biases.\n",
    "- Filters **slide (convolve)** across the image.\n",
    "- Convolution + bias → passed through **non-linearity (ReLU)**.\n",
    "- Output: one matrix per filter → stacked as a **volume**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf27f99f",
   "metadata": {},
   "source": [
    "\n",
    "## Multiple CNN Layers\n",
    "\n",
    "- Next layer convolves the **volume** output of previous layer.\n",
    "- Convolution on volume = sum of convolutions on individual matrices.\n",
    "\n",
    "- Input images are often 3-channel volumes: **R, G, B**.\n",
    "\n",
    "<img src=\"Images/CNN_6.png\" width= 500>\n",
    "\n",
    "## Other CNN Features \n",
    "(Not Covered Here)\n",
    "- **Strides** and **padding**: control filter sliding and image size.\n",
    "- **Pooling**: reduces parameters by downsampling feature maps.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9364fb",
   "metadata": {},
   "source": [
    "\n",
    "# Recurrent Neural Networks (RNNs)\n",
    "\n",
    "## What is an RNN?\n",
    "- A type of neural network used for **sequential data**.\n",
    "- Handles **labeling**, **classification**, and **generation** of sequences.\n",
    "- Commonly used in:\n",
    "  - **Text processing**\n",
    "  - **Speech recognition**\n",
    "  - **Language modeling**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7fe8e1",
   "metadata": {},
   "source": [
    "\n",
    "## Sequence Types\n",
    "- **Labeling**: Predict a class for each time step.\n",
    "- **Classification**: Predict a single class for the full sequence.\n",
    "- **Generation**: Output a related sequence of possibly different length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ecb427",
   "metadata": {},
   "source": [
    "\n",
    "## How RNNs Work\n",
    "- Not feed-forward: contains **loops**.\n",
    "- Each unit has a **state** (memory) $h_{l,u}$.\n",
    "- Each unit receives:\n",
    "  - Output from previous layer $l - 1$\n",
    "  - State from **same layer**, previous time step $t - 1$\n",
    "\n",
    "<img src=\"Images/RNN_6.png\" width= 500>\n",
    "\n",
    "###  Example\n",
    "Let input sequence be:\n",
    "$$\n",
    "X = [x^1, x^2, ..., x^t, ..., x^{length(x)}]\n",
    "$$\n",
    "- $x^t$ is a feature vector at time $t$\n",
    "- Input is processed **one timestep at a time**\n",
    "- If $X$ is a text sentence, then each feature vector $x^t$ represent a word in the sentence at position t.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8b48ca",
   "metadata": {},
   "source": [
    "\n",
    "### State Update Formula\n",
    "\n",
    "For unit $u$ in layer $l$ :\n",
    "$$\n",
    "h_{l,u}^t = g_1(w_{l,u} · x_t + u_{l,u} · h_{l,u}^{t-1} + b_{l,u})\n",
    "$$\n",
    "- $g_1$ is usually $tanh$\n",
    "\n",
    "Output:\n",
    "$$\n",
    "y_1^t = g_2(V_1 · h_1^t + c_{l,u})\n",
    "$$\n",
    "- $g_2$ is typically $softmax$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878a2f92",
   "metadata": {},
   "source": [
    "\n",
    "### Softmax Function\n",
    "$$\n",
    "\\sigma(z) = [\\sigma^{(1)}, ..., \\sigma^{(D)}]\n",
    "$$\n",
    "$$\n",
    "\\sigma(j) = \\frac {exp(z^{(j)})}{\\sum_{k=1}^D exp(z^{(k)})} \n",
    "$$\n",
    "- Generalization of sigmoid\n",
    "- Produces probability distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e00106",
   "metadata": {},
   "source": [
    "\n",
    "## RNN Training\n",
    "- Parameters: $w, u, b, V, C$\n",
    "- Trained via **gradient descent**\n",
    "- Use **Backpropagation Through Time (BPTT)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac076ef",
   "metadata": {},
   "source": [
    "\n",
    "## Problems with Vanilla RNNs\n",
    "1. **Vanishing gradient** (especially with long sequences)\n",
    "2. **Long-term dependencies** are hard to remember\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da74ee83",
   "metadata": {},
   "source": [
    "\n",
    "## Solution - Gated RNNs \n",
    "\n",
    "- Two common types:\n",
    "  - **LSTM (Long Short-Term Memory)**\n",
    "  - **GRU (Gated Recurrent Unit)**\n",
    "- Use **gates** to control memory\n",
    "\n",
    "## GRU: Key Idea\n",
    "- Store, read, and forget info using gates\n",
    "- A GRU unit uses:\n",
    "  - Input $x^t$\n",
    "  - Memory from previous timestep $h_l^{t-1}$\n",
    "\n",
    "## GRU Equations (Minimal Gated Unit)\n",
    "$$\n",
    "h_{l,u}^t = g_1(w_{l,u} · x^t + u_{l,u} · h_l^{t-1} + b_{l,u})\n",
    "$$\n",
    "$$\n",
    "\\tau_{l,u}^t = g_2(m_{l,u} · x^t + o_{l,u} · h^{t-1} + a_{l,u})\n",
    "$$\n",
    "$$\n",
    "h_{l,u}^t = \\tau_{l,u}^t h_{l}^t  + (1- \\tau_{l,u}^t) h_{l}^{t-1}\n",
    "$$\n",
    "\n",
    "**GRU Output :-**\n",
    "$$\n",
    "h_l^t = [h_{l,1}^t, h_{l,2}^t, ..., h_{l,n}^t]\n",
    "$$\n",
    "$$\n",
    "y_l^t = g_3(V_l · h_l^t + c_{l,u}) \n",
    "$$\n",
    "\n",
    "($g_3$ is usually softmax)\n",
    "\n",
    "## Why GRUs Work\n",
    "- **Store info** for many timesteps\n",
    "- **Control** read/write via sigmoid gates (values between 0 and 1)\n",
    "- Avoid vanishing gradients (identity function is part of the design)\n",
    "\n",
    "## Other RNN Variants\n",
    "- **Bi-directional RNNs**\n",
    "- **Attention-based RNNs**\n",
    "- **Sequence-to-sequence (seq2seq)** models\n",
    "- **Recursive** neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c634fd86",
   "metadata": {},
   "source": [
    "\n",
    "## Applications of Sequence Models\n",
    "- Language translation\n",
    "- Chatbots\n",
    "- Speech recognition\n",
    "- Text summarization\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
