---
marp: true
theme: gaia
title: 
math: mathjax

---

# Beyond the Basics

Some **important advanced concepts** (just an intro):

- Topic Modeling  
- Gaussian Processes  
- Generalized Linear Models  
- Probabilistic Graphical Models  
- Markov Chain Monte Carlo  
- Genetic Algorithms  
- Reinforcement Learning  

----
# Topic Modeling  
Discovering themes in large collections of text

### What it is:
- Topic modeling finds **hidden topics** that occur in documents
- It's an **unsupervised learning technique** — no labeled data needed

### Example: Latent Dirichlet Allocation (LDA)
- LDA assumes: each document is a mix of topics, and each topic is a mix of words  

---

- It figures out **which topics** are in which documents by looking at **word co-occurrence patterns**

Real-world uses:
- News categorization  
- Document clustering  
- Recommender systems for articles
---
# Gaussian Processes (GPs)  
Probabilistic regression with uncertainty estimates

### Why GPs are different:
- Most models give a **single prediction**  
- GPs give **a distribution of possible values** — with **confidence bounds**

### How they work:
- Think of GPs as defining a probability over all possible functions

---

- Instead of fitting parameters, GPs "learn" the most likely function that explains your data

**Benefits:**

- Great when your data is **noisy or sparse**  
- Useful in **Bayesian optimization**, robotics, and forecasting

---

# Generalized Linear Models (GLMs)  
A family of models extending linear regression

### What GLMs do:
- Allow the **output (target variable)** to come from different distributions, not just normal
- Use a **link function** to connect the prediction to the input

### Examples:
- Logistic regression → For binary classification

---

- Poisson regression → For count data  
- Gamma regression → For skewed positive values

**Why GLMs matter:**

- They're simple, interpretable, and flexible  
- Great when you want **transparency** in your models

---

# Probabilistic Graphical Models (PGMs)  
Modeling structured relationships between variables

### What they do:
- Use graphs to represent **how variables influence each other**  
- Let us model **uncertainty + structure**

### Some PGMs:
- **Bayesian Networks**: Directed edges = cause → effect

---

- **Markov Random Fields**: Undirected dependencies  
- **CRFs, HMMs**: Specialized for sequence data

**When to use:**
- When variables are not independent  
- When **causal relationships or dependencies** matter  
- Common in NLP, bioinformatics, vision, and time-series

---

# Markov Chain Monte Carlo (MCMC)  
 Sampling from complex probability distributions

### Why we need it:
- Sometimes you can’t compute a distribution directly, but still need to sample from it  
- MCMC lets us do that by using a random walk over possible outcomes

---

### How it works (high level):
- Generates a chain of samples where **each sample depends only on the previous one**  
- Over time, the distribution of these samples **approximates the true distribution**

**Applications:**
- Bayesian inference  
- Deep generative models  
- Physics simulations

---

# Genetic Algorithms (GAs)  
Evolution-based optimization for difficult problems

### What they are:
- Inspired by **natural selection** — survival of the fittest  
- Use operations like **selection, mutation, crossover** to evolve better solutions

---

### When they shine:
- When the objective function is **non-differentiable or unknown**  
- When traditional optimization methods (like gradient descent) fail

 **Use cases:**
- Hyperparameter tuning  
- Game AI  
- Engineering design problems  
- Art/music generation

**Note:** They’re flexible but computationally expensive

---

# Reinforcement Learning (RL)  
Learning from feedback in interactive environments

### What it is:
- A framework where an **agent learns by doing**
- The agent takes actions in an environment and gets **rewards or penalties**

### Goal:
- Learn a policy to **maximize long-term rewards**

---

### Common algorithms:
- Q-Learning, SARSA  
- Deep Q Networks (DQN)  
- Policy Gradient methods

**Applications:**

- Game-playing (e.g., AlphaGo, Dota 2)  
- Robotics and automation  
- Inventory/supply chain optimization  
- Dynamic pricing and financial trading

---

# Conclusion  

###  Covered so far : -

- Core ML concepts, from linear models to deep learning  
- Data preprocessing, model selection, and regularization  
- Feature engineering, ensembles, and neural nets  
- Model evaluation, optimization, and generalization  
- Advanced learning types: metric learning, self-/semi-/active/zero-/one-shot learning  
- Beyond supervised learning: clustering, KDE, recommendation, ranking, RL
