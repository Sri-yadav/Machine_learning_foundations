{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f5ad7c",
   "metadata": {},
   "source": [
    "# Beyond the Basics\n",
    "\n",
    "Some **important advanced concepts** (just an intro):\n",
    "\n",
    "- Topic Modeling  \n",
    "- Gaussian Processes  \n",
    "- Generalized Linear Models  \n",
    "- Probabilistic Graphical Models  \n",
    "- Markov Chain Monte Carlo  \n",
    "- Genetic Algorithms  \n",
    "- Reinforcement Learning  \n",
    "\n",
    "----\n",
    "# Topic Modeling  \n",
    "Discovering themes in large collections of text\n",
    "\n",
    "### What it is:\n",
    "- Topic modeling finds **hidden topics** that occur in documents\n",
    "- It's an **unsupervised learning technique** — no labeled data needed\n",
    "\n",
    "### Example: Latent Dirichlet Allocation (LDA)\n",
    "- LDA assumes: each document is a mix of topics, and each topic is a mix of words  \n",
    "\n",
    "---\n",
    "\n",
    "- It figures out **which topics** are in which documents by looking at **word co-occurrence patterns**\n",
    "\n",
    "Real-world uses:\n",
    "- News categorization  \n",
    "- Document clustering  \n",
    "- Recommender systems for articles\n",
    "---\n",
    "# Gaussian Processes (GPs)  \n",
    "Probabilistic regression with uncertainty estimates\n",
    "\n",
    "### Why GPs are different:\n",
    "- Most models give a **single prediction**  \n",
    "- GPs give **a distribution of possible values** — with **confidence bounds**\n",
    "\n",
    "### How they work:\n",
    "- Think of GPs as defining a probability over all possible functions\n",
    "\n",
    "---\n",
    "\n",
    "- Instead of fitting parameters, GPs \"learn\" the most likely function that explains your data\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- Great when your data is **noisy or sparse**  \n",
    "- Useful in **Bayesian optimization**, robotics, and forecasting\n",
    "\n",
    "---\n",
    "\n",
    "# Generalized Linear Models (GLMs)  \n",
    "A family of models extending linear regression\n",
    "\n",
    "### What GLMs do:\n",
    "- Allow the **output (target variable)** to come from different distributions, not just normal\n",
    "- Use a **link function** to connect the prediction to the input\n",
    "\n",
    "### Examples:\n",
    "- Logistic regression → For binary classification\n",
    "\n",
    "---\n",
    "\n",
    "- Poisson regression → For count data  \n",
    "- Gamma regression → For skewed positive values\n",
    "\n",
    "**Why GLMs matter:**\n",
    "\n",
    "- They're simple, interpretable, and flexible  \n",
    "- Great when you want **transparency** in your models\n",
    "\n",
    "---\n",
    "\n",
    "# Probabilistic Graphical Models (PGMs)  \n",
    "Modeling structured relationships between variables\n",
    "\n",
    "### What they do:\n",
    "- Use graphs to represent **how variables influence each other**  \n",
    "- Let us model **uncertainty + structure**\n",
    "\n",
    "### Some PGMs:\n",
    "- **Bayesian Networks**: Directed edges = cause → effect\n",
    "\n",
    "---\n",
    "\n",
    "- **Markov Random Fields**: Undirected dependencies  \n",
    "- **CRFs, HMMs**: Specialized for sequence data\n",
    "\n",
    "**When to use:**\n",
    "- When variables are not independent  \n",
    "- When **causal relationships or dependencies** matter  \n",
    "- Common in NLP, bioinformatics, vision, and time-series\n",
    "\n",
    "---\n",
    "\n",
    "# Markov Chain Monte Carlo (MCMC)  \n",
    " Sampling from complex probability distributions\n",
    "\n",
    "### Why we need it:\n",
    "- Sometimes you can’t compute a distribution directly, but still need to sample from it  \n",
    "- MCMC lets us do that by using a random walk over possible outcomes\n",
    "\n",
    "---\n",
    "\n",
    "### How it works (high level):\n",
    "- Generates a chain of samples where **each sample depends only on the previous one**  \n",
    "- Over time, the distribution of these samples **approximates the true distribution**\n",
    "\n",
    "**Applications:**\n",
    "- Bayesian inference  \n",
    "- Deep generative models  \n",
    "- Physics simulations\n",
    "\n",
    "---\n",
    "\n",
    "# Genetic Algorithms (GAs)  \n",
    "Evolution-based optimization for difficult problems\n",
    "\n",
    "### What they are:\n",
    "- Inspired by **natural selection** — survival of the fittest  \n",
    "- Use operations like **selection, mutation, crossover** to evolve better solutions\n",
    "\n",
    "---\n",
    "\n",
    "### When they shine:\n",
    "- When the objective function is **non-differentiable or unknown**  \n",
    "- When traditional optimization methods (like gradient descent) fail\n",
    "\n",
    " **Use cases:**\n",
    "- Hyperparameter tuning  \n",
    "- Game AI  \n",
    "- Engineering design problems  \n",
    "- Art/music generation\n",
    "\n",
    "**Note:** They’re flexible but computationally expensive\n",
    "\n",
    "---\n",
    "\n",
    "# Reinforcement Learning (RL)  \n",
    "Learning from feedback in interactive environments\n",
    "\n",
    "### What it is:\n",
    "- A framework where an **agent learns by doing**\n",
    "- The agent takes actions in an environment and gets **rewards or penalties**\n",
    "\n",
    "### Goal:\n",
    "- Learn a policy to **maximize long-term rewards**\n",
    "\n",
    "---\n",
    "\n",
    "### Common algorithms:\n",
    "- Q-Learning, SARSA  \n",
    "- Deep Q Networks (DQN)  \n",
    "- Policy Gradient methods\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "- Game-playing (e.g., AlphaGo, Dota 2)  \n",
    "- Robotics and automation  \n",
    "- Inventory/supply chain optimization  \n",
    "- Dynamic pricing and financial trading\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd55b674",
   "metadata": {},
   "source": [
    "###  Covered so far : -\n",
    "\n",
    "- Core ML concepts, from linear models to deep learning  \n",
    "- Data preprocessing, model selection, and regularization  \n",
    "- Feature engineering, ensembles, and neural nets  \n",
    "- Model evaluation, optimization, and generalization  \n",
    "- Advanced learning types: metric learning, self-/semi-/active/zero-/one-shot learning  \n",
    "- Beyond supervised learning: clustering, KDE, recommendation, ranking, RL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e453f8",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "This projects gives a structured and practical overview of machine learning based on the book, \"The hundred pages of machine learning\" by Andriy Burlov. Key concepts are explored through concise explainations and interactive jupyter notebooks that demonstrates real world applications.\n",
    "\n",
    "Overall it provides a clear understanding of machine workflows from data processing to model evaluation together with advanced techniques, building a strong foundation for deeper exploration."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
