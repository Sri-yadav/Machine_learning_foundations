{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "556cc3a9",
   "metadata": {},
   "source": [
    "# Fundamental Algorithms\n",
    "(Five most commonly used **supervised learning algorithms** will be explained here). These are :-\n",
    "\n",
    "**1. Linear Regression   \n",
    "2. Logistic Regression   \n",
    "3. Decision Tree Learning  \n",
    "4. Support Vector Machine  \n",
    "5. k Nearest Neighbors**  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14625e19",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Linear Regression \n",
    "\n",
    "A learning algorithm that learns a model which is a linear combination of features of the input examples and uses the model to output real-valued target when an input in given.\n",
    "\n",
    "- Commanly used for regression task\n",
    "- Simple and effective\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "\n",
    "- **Given** : *Dataset* of $N$ labelled examples - $\\{x_i,y_i\\}_{i=1}^N$ \n",
    "\n",
    "    **$x_i$** - D-dimensional feature vector , $y_i$ - real-valued target\n",
    "\n",
    "- **Goal** : Build a model ($f_{\\boldsymbol{w},b}$) given by,\n",
    "\n",
    "    $f_{\\boldsymbol{w},b}(x)= \\boldsymbol{w^*x} + b^*$\n",
    "\n",
    "    where $w^*$ and $b^*$  the optimal values that makes the most accurate predictions\n",
    "\n",
    "\n",
    "    **Predict**: Unknown $y$ using : **$y=f_{\\boldsymbol{w},b}(x_{new})$** ; \n",
    "\n",
    "- **Note**:-\n",
    "    - It predicts real-values not classes.\n",
    "    - It tries to decrease the error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbc0374",
   "metadata": {},
   "source": [
    "\n",
    "**Visualization**  \n",
    "\n",
    "<img src=\"Images/Linear_regression_1D.png\" width=\"500\">\n",
    "\n",
    "Regression model for :-\n",
    "- $1D$ - a line\n",
    "- $2D$ - a plane\n",
    "- $D$ - dimensions - a hyperplane of $D$-dimensions\n",
    "\n",
    "**Note:** It is different from SVM (which has hyperplane of $D - 1$ dimensions).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c313be",
   "metadata": {},
   "source": [
    "\n",
    "## Solution\n",
    "\n",
    "- **Objective function** - function that we minimize or maximize to find the optimal values of the original function (the model).\n",
    "\n",
    "- We want to find the optimal values of $w$ and $b$ by minimising the prediction error. It is given by the **cost function** (or empirical risk).\n",
    "\n",
    "    $$\n",
    "    \\frac{1}{N}\\sum_{i=1}^N(f{w,b}(x_i)-y_1)^2\n",
    "    $$\n",
    "\n",
    "    where $(f{w,b}(x_i)-y_1)^2$ is called the loss function (or squared error loss)\n",
    "\n",
    "- **Loss function** is a measure of how wrong the predicted value is from the real value (i.e penalty for misclassification).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64139829",
   "metadata": {},
   "source": [
    "\n",
    "- **Why Linear** ?\n",
    "    - It's simple.\n",
    "    - Rarely **overfits**.\n",
    "\n",
    "- **Why use squared loss? Why not absolute or cubed?**\n",
    "\n",
    "    - Absoulte error? Cubed error? All are valid. But we use squared because \n",
    "        - It exaggerates larger errors - so penalize big mistakes more\n",
    "\n",
    "        - Has smooth , continuous derivative - Optimization becomes easier. Simpler to compute closed form solution using linear algebra.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9f1a5f",
   "metadata": {},
   "source": [
    "\n",
    "- **Gradient descent** : a complex numerical optimization method (discussed later) that can help in finding the optimal values of the parameters.\n",
    "\n",
    "- **Overfitting** (high accuracy on training data, poor performance on unseen data)\n",
    "\n",
    "<img src=\"Images/Linear_regression_1D.png\" width=\"550\">\n",
    "<img src=\"Images/Overfitting.png\" width=\"500\"> \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Optimization \n",
    "\n",
    "- Optimization: Minimizing cost of a function\n",
    "- can be done by setting the gradient of the cost function to zero or other methods like Gradient descent.\n",
    "- This gives optimal values - $w$* , $b$* in case of Linear Regression.\n",
    "\n",
    "    $$\n",
    "    C(w,b) = \\frac{1}{N}\\sum_{i=1}^N(f_{w,b}(x_i)-y_1)^2  \n",
    "    $$\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial{C}}{\\partial{w}}= 0 ; \\frac{\\partial{C}}{\\partial{b}}= 0\n",
    "    $$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c02188",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Logistic Regression\n",
    "\n",
    "- Not really a regression, but classification. \n",
    "- Named so - because its mathematical form is similar to linear regression.\n",
    "\n",
    "## Problem Statement \n",
    "\n",
    "For binary classification (can also be extended to multiclass)\n",
    "\n",
    "- **Given** : *Dataset* of $N$ labelled examples - $\\{x_i,y_i\\}_{i=1}^N$ where $y_i\\in\\{0,1\\}$ \n",
    "\n",
    "- **Goal** : Build a linear model that predicts whether an input belongs to class 0 or 1.\n",
    "\n",
    "    - Problem: Can't use linear expression like **$wx_i + b$** as it spans from -$\\infty$ to +$\\infty$ while $y_i$ only has two values.\n",
    "\n",
    "    - Solution: Use a simple continuous function whose codomain is (0,1)\n",
    "    \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eac5fb",
   "metadata": {},
   "source": [
    "\n",
    "## Logistic Regression Model\n",
    "\n",
    "Defined as, \n",
    "\n",
    "$$\n",
    "\n",
    "f_{w,b}(x)= \\frac{1}{1 + e^{-(wx + b)}}\n",
    "\n",
    "$$\n",
    "\n",
    "- a logistic function that give values between 0 and 1 \n",
    "- can be interpreted as $Pr(y=1\\mid x)$\n",
    "- if $f(x) >= 0.5$, predict class 1\n",
    "- if $f(x) < 0.5$, predict class 0\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3a6fc2",
   "metadata": {},
   "source": [
    "\n",
    "## Solution\n",
    "\n",
    "(Again we want to find the optimal values $w^*$ & $b^*$)\n",
    "\n",
    "- Unlike linear regression ( which minimizes **MSE**), \n",
    " Logistic regression uses **maximum likelihood** estimation (which maximize the likelihood of the training data according to our model). Given by, \n",
    "\n",
    " $$\n",
    " L_{w,b}= \\prod_{i=1}^N f_{w,b}(x_i)^{y_i} (1-f_{w,b}(x_i))^{1-y_i} \n",
    " $$\n",
    " $$\n",
    " f_{w,b}(x)\\;when\\;y_i=1 , (1-f_{w,b}(x))\\;otherwise\n",
    " $$\n",
    "\n",
    "- In practice, we take *log-likelihood* instead because of :-\n",
    "    - presence of $exp$ function, taking $log$ makes it easier to compute\n",
    "    - turns products into sum\n",
    "    - leads to same optimization\n",
    "\n",
    "$$\n",
    "\\log L_{w,b}= \\sum_{i=1}^N y_i\\;lnf_{w,b}(x) + (1-y_i)\\;ln(1-f_{w,b}(x))\n",
    "$$\n",
    "\n",
    "## Optimization\n",
    "\n",
    "- Unlike linear regression, it doesn't have a closed forms solution\n",
    "\n",
    "- **Gradient descent** is used to optimise such problems\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f20d08e",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Decision Tree Learning\n",
    "\n",
    "- A learning algorithm that makes a **non-parametric model** given by a decision tree to predict which class the input feature vector belongs to.\n",
    "- **Decision Tree**: \n",
    "    - an acyclic graph\n",
    "    - where at each node of the graph a specific feature $j$ of the feature vector is examined.\n",
    "    - if the value of the feature is below threshold, then left branch is followed. Else, the right branch is followed. \n",
    "    - Leaf node represents class predictions.\n",
    "\n",
    "<img src=\"Images/Decision_Tree_3.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c218857",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Problem statement\n",
    "\n",
    "**Given**: *Dataset* of $N$ labelled examples - $\\{x_i,y_i\\}_{i=1}^N$ where $y_i\\in\\{0,1\\}$ \n",
    "\n",
    "**Goal**: To build a decision tree (model) that can predict the label of the the class to which the input feature vector belongs to.\n",
    "\n",
    "- The tree is built from the data\n",
    "- Initially, all the training data is at the root node.\n",
    "- Repeatedly split the data by choosing the best feature & threshold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa652e",
   "metadata": {},
   "source": [
    "\n",
    "## Model function\n",
    "\n",
    "- Initially, start with a constant model $f_{ID3}^S$ given by,\n",
    "$$\n",
    "f_{ID3}^S = \\frac{1}{\\mid S \\mid} \\sum_{(x,y)\\in S}y\n",
    "$$\n",
    "\n",
    "- Predicts same value for any input $x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77357807",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Splitting the Data\n",
    "\n",
    "- Try all features $j = 1, \\dots, D$ and all thresholds $t$.\n",
    "- Split into:\n",
    "  - $S^- = \\{(x, y) \\mid x^{(j)} < t\\}$\n",
    "  - $S^+ = \\{(x, y) \\mid x^{(j)} \\geq t\\}$\n",
    "- Choose split that gives **lowest entropy**.\n",
    "\n",
    "#### Entropy\n",
    "- measure of uncertainty about a random variable. Given by,\n",
    "$$\n",
    "H(S) = -f_{ID3} \\ln f_{ID3} - (1 - f_{ID3}) \\ln(1 - f_{ID3})\n",
    "$$\n",
    "\n",
    "- Minimum Entropy = 0 ; all labels are same (no uncertainty).\n",
    "- Maximum Entropy = 1 ; labels are split 50-50 (maximum uncertainty).\n",
    "\n",
    "- Entropy after a Split :\n",
    "    $$\n",
    "    H(S^-, S^+) = \\frac{|S^-|}{|S|} H(S^-) + \\frac{|S^+|}{|S|} H(S^+)\n",
    "    $$\n",
    "\n",
    "    - Weighted average of entropies of both subsets.\n",
    "    - Best split minimizes this value.\n",
    "\n",
    "\n",
    "### Stop Splitting\n",
    "\n",
    "Tree growth stops at a leaf node, if :\n",
    "\n",
    "- All labels in node are same.\n",
    "- No good split is found.\n",
    "- Entropy reduction < $\\varepsilon$ (found experimentally).\n",
    "- Tree reaches maximum depth $d$ (found experimentally).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6924c05",
   "metadata": {},
   "source": [
    "\n",
    "## Optimization Criterion\n",
    "\n",
    "- The algorithm implicitly connects to optimizing a  **log-likelihood** :\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{i=1}^N y_i \\ln f_{ID3}(x_i) + (1 - y_i) \\ln(1 - f_{ID3}(x_i))\n",
    "$$\n",
    "\n",
    " ## Limitations of ID3\n",
    "\n",
    "- Makes local decisions, doesn’t guarantee globally optimal tree.\n",
    "- Can be improved using Backtracking (but can possibly increase the time taken to build the model) .\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4a71b",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Support Vector Machines (SVMs)\n",
    "\n",
    "- Finds the best hyperplane that separates classes with the widest margin. \n",
    "\n",
    "- We’ve already seen the basic idea. Now we explore how SVM handles :  \n",
    "  1. **Noisy data** (imperfect separation)  \n",
    "  2. **Non-linear boundaries** (inherently complex patterns)  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf6ca03",
   "metadata": {},
   "source": [
    "\n",
    "## Hard-Margin SVM\n",
    "(already explained before)\n",
    "\n",
    "- Constraints for perfectly separable data:  \n",
    "  $$\n",
    "  \\begin{cases}\n",
    "  w x_i - b \\ge +1 & \\text{if } y_i = +1 \\\\[6pt]\n",
    "  w x_i - b \\le -1 & \\text{if } y_i = -1\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "- **Objective:** maximise margin  → minimise $\\tfrac12 \\|w\\|^2$.  \n",
    "  $$\n",
    "  \\min_{w,b}\\; \\tfrac12 \\|w\\|^2\n",
    "  \\quad\\text{s.t.}\\quad\n",
    "  y_i\\bigl(w x_i - b\\bigr) \\ge 1\\;\\; \\forall i\n",
    "  $$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefa8174",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Dealing with Noise \n",
    "\n",
    "(Linearly *Non-Separable* by Outliers)\n",
    "\n",
    "- Some points cannot be separated perfectly due to noise or mislabeling.  \n",
    "- Hard-margin SVM fails to find a perfect hyperplane.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32028e68",
   "metadata": {},
   "source": [
    "\n",
    "### Soft-Margin & Hinge Loss\n",
    "\n",
    "- **Hinge loss:**  \n",
    "  $$\n",
    "  \\max\\!\\bigl(0,\\; 1 - y_i(w^\\top x_i - b)\\bigr)\n",
    "  $$\n",
    "\n",
    "  It is zero when the constraints are satisfied (i.e $wx_i$ lies on the correct side)\n",
    "\n",
    "- **New cost function** :  \n",
    "  $$\n",
    "  \\min_{w,b}\\; C\\|w\\|^2 + \\frac1N \\sum_{i=1}^{N}\n",
    "           \\max\\!\\bigl(0,\\; 1 - y_i(w x_i - b)\\bigr)\n",
    "  $$\n",
    "\n",
    "- **Hyper-parameter \\(C\\):**\n",
    "\n",
    "    Chosen experimentally\n",
    "    - High \\(C\\) → penalize errors, smaller margin  \n",
    "  - Low \\(C\\) → allow some errors, larger margin → better generalisation\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6707c79e",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Dealing with Inherent Non-Linearity\n",
    "\n",
    "- Even noise-free data may need a **curved** boundary \n",
    "- cannot be seperated by a hyperplane in the original space.\n",
    "\n",
    "### Solution : Map to Higher Dimension\n",
    "\n",
    "- Use a mapping $\\phi : x \\to \\phi(x)$ where $\\phi(x)$ is a vector in higher dimension so that data becomes linearly separable. \n",
    "\n",
    "- Example : for 2D point $(q,p)$:  \n",
    "  $$\n",
    "  \\Phi(q,p) = \\bigl(q^2,\\; \\sqrt2\\,qp,\\; p^2\\bigr)\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9efeb6",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"Images/SVM(3D).png\" width= 525> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1fd078",
   "metadata": {},
   "source": [
    "<img src=\"Images/SVM (2D).png\" width= 500>\n",
    "\n",
    "\n",
    "- In new space, a plane can separate the classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb6655",
   "metadata": {},
   "source": [
    "\n",
    "## SVM Optimization\n",
    "\n",
    "- Optimization uses **Lagrange multipliers**  \n",
    "- Dual form (simplified):\n",
    "\n",
    "$$\n",
    "\\max_{\\alpha} \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=1,j=1}^N \\alpha_i \\alpha_j y_i y_j (x_i.x_j)\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "$$\n",
    "\\sum_{i=1}^{N} \\alpha_i y_i = 0, \\quad \\alpha_i \\geq 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29364872",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Kernel Trick\n",
    "\n",
    "- For data in higher dimensional form, we nedd to replace $x.x'$ by $\\Phi(x_i) \\Phi(x_k)$. It would it costly to do.\n",
    "- To avoid computation of $\\Phi(x)$ explicitly.  \n",
    "    - Replace dot-product $\\Phi(x_i) \\Phi(x_k)$ by kernel $k(x_i,x_k)$ where $k(x_i,x_k)$ gives the same result as $\\Phi(x_i) \\Phi(x_k)$ \n",
    "\n",
    "- **Common kernels**: \n",
    "\n",
    "  - Polynomial: $k(x,x') = (x.x')^n$  \n",
    "  - RBF: $k(x,x') = \\exp \\left(\\frac{-\\|x-x'\\|^2} {2\\sigma^2}\\right)$\n",
    "\n",
    "    where $\\|x-x'\\|^2$ is the squared **Euclidean distance** between two feature vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356904a2",
   "metadata": {},
   "source": [
    "\n",
    "## Final Decision Function\n",
    "\n",
    "$$\n",
    "f(x) = \\operatorname{sign}\\Big(\\sum_{i \\in SV} \\alpha_i y_i k(x_i, x) - b\\Big)\n",
    "$$\n",
    "\n",
    "- Only **support vectors** (with $\\alpha_i > 0$) influence the decision.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aedea8f",
   "metadata": {},
   "source": [
    "\n",
    "# 5. k-Nearest Neighbors (kNN)\n",
    "\n",
    "- **Non-parametric** and **instance-based** algorithm  \n",
    "- Does **not discard** training data instead uses it as a model  \n",
    "- For a new input $x$:  \n",
    "  - Finds $k$ closest training examples  \n",
    "  - **Returns:**  \n",
    "    - Majority label (classification)  \n",
    "    - Average label (regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10732541",
   "metadata": {},
   "source": [
    "\n",
    "## How Distance Is Measured\n",
    "\n",
    "- Requires a ***distance metric***\n",
    "- Common choices:\n",
    "  - **Euclidean distance**\n",
    "  - **Negative cosine similarity**\n",
    "  - **Chebyshev distance**\n",
    "  - **Mahalanobis distance**\n",
    "  - **Hamming distance**\n",
    "- *Distance metric* and $k$ are **hyperparameters**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a1081",
   "metadata": {},
   "source": [
    "\n",
    "## Cosine Similarity\n",
    "\n",
    "$$\n",
    "\\text{s}(x_i, x_k) = \\cos(\\theta) = \n",
    "\\frac{\\sum_{j=1}^{D} x_i^{(j)} x_k^{(j)}}{\n",
    "\\sqrt{\\sum_{j=1}^{D} (x_i^{(j)})^2} \\cdot\n",
    "\\sqrt{\\sum_{j=1}^{D} (x_k^{(j)})^2}\n",
    "}\n",
    "$$\n",
    "\n",
    "- 1 → same direction  \n",
    "- 0 → orthogonal  \n",
    "- –1 → opposite direction  \n",
    "- Multiply by –1 to use as **distance**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d539de8",
   "metadata": {},
   "source": [
    "## Cost function?\n",
    "- KNN was introduced as a lazy learning algorithm. There was no cost function defined or minimized during training — because there is no training.\n",
    "- An implicit cost function **Given by Li & Yang, 2003**\n",
    "\n",
    "### Prediction as Local Linear Classifier\n",
    "\n",
    "Assume:\n",
    "- Binary classification: $y \\in \\{0, 1\\}$  \n",
    "- Normalized vectors\n",
    "\n",
    "$$\n",
    "w_x = \\sum_{(x', y') \\in R_k(x)} y' x'\n",
    "$$\n",
    " where $R_k(x)$ is the set of k nearest neighbors to the input example $x$\n",
    " \n",
    "- Sum of feature vectors of **positive-labeled** neighbors  \n",
    "- Decision based on cosine similarity:\n",
    "$$\n",
    "\\text{predict} = \\text{sign}(x w_x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b721b44c",
   "metadata": {},
   "source": [
    "\n",
    "### Cost Function\n",
    "**Given by Li & Yang, 2003**\n",
    "\n",
    "$$\n",
    "L = -\\sum_{(x', y') \\in R_k(x)} y' x' w_x + \\frac{1}{2} \\|w_x\\|^2\n",
    "$$\n",
    "\n",
    "- Optimizing this gives same $w_x$ as before  \n",
    "- kNN approximates a **local linear model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e487140a",
   "metadata": {},
   "source": [
    "\n",
    "*(Using all these five learning models for predicting)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c006bf9",
   "metadata": {},
   "source": [
    "# Python Code : -\n",
    "\n",
    "Using all those five learning models which we discussed earlier for predicting.\n",
    "\n",
    "**Using buit-in Iris dataset from Scikit-learn**\n",
    "\n",
    "- This dataset contains 150 samples of iris flowers.\n",
    "\n",
    "- Each sample has 4 features:\n",
    "    - Sepal length (cm)\n",
    "    - Sepal width (cm)\n",
    "    - Petal length (cm)\n",
    "    - Petal width (cm)\n",
    "\n",
    "- Each sample belongs to one of 3 species:\n",
    "    - Setosa (label = 0)\n",
    "    - Versicolor (label = 1)\n",
    "    - Virginica (label = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1ef574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "   target  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "\n",
    "print(df.head()) # View dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bccb3a",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d3bb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and labels\n",
    "X = df.drop('target', axis=1)  # Feature data\n",
    "y = df['target']               # Labels\n",
    "\n",
    "# Split into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data (80% for training, 20% for testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8972e2d6",
   "metadata": {},
   "source": [
    "# Code to Train and Predict\n",
    "\n",
    "## 1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0f4ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Create the model\n",
    "LR_model = LinearRegression()\n",
    "\n",
    "# Train the model on training data\n",
    "LR_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred_lin = LR_model.predict(X_test)\n",
    "\n",
    "# Since it's output is continuous numbers, we round them to nearest class\n",
    "y_pred_round = np.round(y_pred_lin).astype(int)\n",
    "\n",
    "# Clip the predictions to stay within valid label range [0, 2]\n",
    "y_pred_clipped = np.clip(y_pred_round, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeee13d3",
   "metadata": {},
   "source": [
    "### Accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63cc9015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Linear-Regression : 0.9333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        12\n",
      "  versicolor       0.89      0.89      0.89         9\n",
      "   virginica       0.89      0.89      0.89         9\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.93      0.93      0.93        30\n",
      "weighted avg       0.93      0.93      0.93        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_lin = accuracy_score(y_test, y_pred_clipped)\n",
    "print(\"Accuracy of Linear-Regression :\", acc_lin)\n",
    "\n",
    "# See a per-class breakdown\n",
    "print(classification_report(y_test, y_pred_clipped, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1fe29b",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Accuracy: 93.3%\n",
    "Predicted labels: Mostly correct, but not perfect\n",
    "Evaluation: Misclassifies some versicolor/virginica samples.\n",
    "\n",
    "It is not meant for classification but for continuous values. We are rounding these values to nearest classes.\n",
    "That's why : \n",
    "- It is working for classes that are well seperated numerically (Setosa).\n",
    "- Stuggles when class boundaries arent linear. (btw versicolor and virginica)\n",
    "\n",
    "**Shouldn't be used for classification. Worked here only because the the probelm is simple and setosa is well seperated.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7bdcab",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7b51535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create the model\n",
    "log_reg = LogisticRegression(\n",
    "    max_iter=200,      # allow enough iterations to converge\n",
    ")\n",
    "\n",
    "# Train (fit) on the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred_log = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16e17a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic-Regression : 0.9666666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        12\n",
      "  versicolor       1.00      0.89      0.94         9\n",
      "   virginica       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.96      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "acc_log = accuracy_score(y_test, y_pred_log)\n",
    "print(\"Accuracy of Logistic-Regression :\", acc_log)\n",
    "\n",
    "# See a per-class breakdown\n",
    "print(classification_report(y_test, y_pred_log, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86266d4",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Accuracy: 96.7%\n",
    "Slighly low recall for Virgenica\n",
    "\n",
    "**Why this performance?**\n",
    "Logistic regression draws linear boundaries between classes.\n",
    "\n",
    "Works very well when classes are linearly separable — and Iris almost is (except versicolor vs virginica overlap).\n",
    "\n",
    "What this tells about the data?\n",
    "The dataset is mostly linearly separable, except a few cases between versicolor and virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5130856b",
   "metadata": {},
   "source": [
    "## 3. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "938b4bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create the model\n",
    "dec_tree = DecisionTreeClassifier(\n",
    "    criterion=\"gini\",   # or \"entropy\"\n",
    "    max_depth=3,              # prevent overfitting\n",
    "    min_samples_split=4,      # don’t split unless ≥4 samples\n",
    "    min_samples_leaf=2,       # each leaf should have ≥2 samples\n",
    "    random_state=30     # reproducible splits\n",
    ")\n",
    "\n",
    "# Fit (train) on the training data\n",
    "dec_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred_dt = dec_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eed01c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision-Tree: 0.9333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        12\n",
      "  versicolor       0.89      0.89      0.89         9\n",
      "   virginica       0.89      0.89      0.89         9\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.93      0.93      0.93        30\n",
      "weighted avg       0.93      0.93      0.93        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(\"Accuracy of Decision-Tree:\", acc_dt)\n",
    "print(classification_report(y_test, y_pred_dt, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a397ea6b",
   "metadata": {},
   "source": [
    "### Evaluation of this model\n",
    "\n",
    "Accuracy: 93.3%\n",
    "\n",
    "Why this performance?\n",
    "Trees split based on thresholds of feature values (like petal length < x).\n",
    "In Iris, a few splits are enough to classify most points correctly — but they may still make errors on overlapping samples.\n",
    "\n",
    "**What it says about the data:**\n",
    "\n",
    "Decision boundaries based on specific feature thresholds are enough to distinguish most classes.\n",
    "But the noise/overlap between class 1 and 2 causes misclassifications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56105e",
   "metadata": {},
   "source": [
    "## 4. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "566bc839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create the model\n",
    "svm = SVC(kernel='linear', C=1.0, random_state=30)\n",
    "\n",
    "# Train the model\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_svm = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "280140b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM : 0.9666666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        12\n",
      "  versicolor       1.00      0.89      0.94         9\n",
      "   virginica       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.96      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "acc_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"Accuracy of SVM :\", acc_svm)\n",
    "print(classification_report(y_test, y_pred_svm, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352abbaa",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Accuracy: 96.7% - same as logistic regression\n",
    "\n",
    "Same interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb38f577",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00f06e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create the model\n",
    "k_nn = KNeighborsClassifier(n_neighbors=5)  # 5 nearest neighbors\n",
    "\n",
    "# Train the model (just memorizes training data)\n",
    "k_nn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_k_nn = k_nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "776c66e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of k-Nearest Neighbors : 0.9333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        12\n",
      "  versicolor       1.00      0.78      0.88         9\n",
      "   virginica       0.82      1.00      0.90         9\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.94      0.93      0.92        30\n",
      "weighted avg       0.95      0.93      0.93        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "acc_k_nn = accuracy_score(y_test, y_pred_k_nn)\n",
    "print(\"Accuracy of k-Nearest Neighbors :\", acc_k_nn)\n",
    "print(classification_report(y_test, y_pred_k_nn, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3afdd2",
   "metadata": {},
   "source": [
    "Accuracy: 93.3%\n",
    "\n",
    "What this tells about the data:\n",
    "\n",
    "Local neighborhood for Setosa is clean → perfect accuracy\n",
    "\n",
    "But Versicolor vs Virginica overlap in feature space → confusion in neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901d5bcd",
   "metadata": {},
   "source": [
    "# Regression Problem\n",
    "\n",
    "Similar to the previous one, but here we are trying to solve a regression problem instead of classification.\n",
    "\n",
    "We are using another built-in real world regression dataset - California_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99a2c6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model   MSE  R² Score\n",
      "0  Linear Regression  0.56      0.58\n",
      "1      Decision Tree  0.50      0.62\n",
      "2      KNN Regressor  0.43      0.67\n",
      "3   SVR (RBF Kernel)  0.36      0.73\n"
     ]
    }
   ],
   "source": [
    "# Combined code for training four algorithms\n",
    " \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "\n",
    "# Optional: Standardize features (important for SVR, KNN, etc.)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=30),\n",
    "    \"KNN Regressor\": KNeighborsRegressor(n_neighbors=5),\n",
    "    \"SVR (RBF Kernel)\": SVR(kernel='rbf')\n",
    "}\n",
    "\n",
    "# Train and evaluate\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"MSE\": round(mse, 2),\n",
    "        \"R² Score\": round(r2, 2)\n",
    "    })\n",
    "\n",
    "# Show results\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6cc518",
   "metadata": {},
   "source": [
    "# Evaluation : -\n",
    "\n",
    "1. SVR (RBF Kernel) – Best MSE = 0.36\n",
    "\n",
    "- Most accurate model overall.\n",
    "\n",
    "- This low MSE means the model is consistently close to the true values.\n",
    "\n",
    "- The RBF kernel captures nonlinear relationships smoothly.\n",
    "\n",
    "- Great when the data has complex, curved, or indirect patterns.\n",
    "\n",
    "\n",
    "**The data likely has nonlinear dependencies.**\n",
    "\n",
    "2. KNN Regressor – MSE = 0.43\n",
    "\n",
    "- Second-best performance and very close to SVR.\n",
    "\n",
    "- Predicts values based on averages of nearby training samples.\n",
    "\n",
    "**The dataset has some local structure or clusters where nearby samples tend to have similar outputs.**\n",
    "\n",
    "3. Decision Tree – MSE = 0.50\n",
    "\n",
    "- Less precise than KNN or SVR.\n",
    "\n",
    "- Predicts using stepwise thresholds, which might miss finer variations.\n",
    "\n",
    "- Can create jumps in predictions instead of smooth transitions.\n",
    "\n",
    "**The data has some natural thresholds or decision points. Not good but atleast better than Linear Regression**\n",
    "\n",
    "4. Linear Regression – MSE = 0.56\n",
    "\n",
    "- Worst performer here.\n",
    "\n",
    "- Assumes a straight-line relationship between inputs and target.\n",
    "\n",
    "- MSE is higher because it cannot handle curved or complex patterns.\n",
    "\n",
    "- Still not terrible — means some linear trend is present, just not enough.\n",
    "\n",
    "\n",
    "**The model is too simple for this dataset — linear assumptions don't capture the full pattern.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
